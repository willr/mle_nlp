{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import unidecode\n",
    "# import contractions\n",
    "import re\n",
    "# from word2number import w2n\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "# Keras package\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, LSTM, Lambda, Bidirectional\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize global variables\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 60  \n",
    "MAX_NUM_WORDS = 200000  # There are about 201000 unique words in training dataset, 200000 is enough for tokenization\n",
    "EMBEDDING_DIM = 300  # word-embedded-vector dimension(300 is for 'glove.42B.300d')\n",
    "N_HIDDEN = 512\n",
    "N_DENSE = 256\n",
    "\n",
    "DROPOUT_RATE_LSTM = 0.10 # drop-out possibility, random set to avoid outfitting\n",
    "DROPOUT_RATE_DENSE = 0.15\n",
    "\n",
    "ACTIVE_FUNC = 'relu'\n",
    "VERSION = 'bilstm'\n",
    "\n",
    "PATH_TO_GLOVE_FILE = './data/glove.42B.300d.txt'\n",
    "# PATH_TO_GLOVE_FILE = './data/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes Hidden: 512\n",
      "Nodes Dense: 256\n",
      "Dropout Rate LSTM: 0.1\n",
      "Dropout Rate Dense: 0.15\n"
     ]
    }
   ],
   "source": [
    "print(f'Nodes Hidden: {N_HIDDEN}')\n",
    "print(f'Nodes Dense: {N_DENSE}')\n",
    "\n",
    "print(f'Dropout Rate LSTM: {DROPOUT_RATE_LSTM}')\n",
    "print(f'Dropout Rate Dense: {DROPOUT_RATE_DENSE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create word embedding dictionary\n",
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Create word embedding dictionary from 'glove.840B.300d.txt', {key:value} is {word: glove vector(300,)}\n",
    "print('Create word embedding dictionary')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(PATH_TO_GLOVE_FILE) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/questions.csv.zip')\n",
    "df.head()\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  qid1  qid2                                          question1  \\\n",
      "5    5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
      "7    7    15    16                     How can I be a good geologist?   \n",
      "11  11    23    24        How do I read and find my YouTube comments?   \n",
      "12  12    25    26               What can make Physics easy to learn?   \n",
      "13  13    27    28        What was your first sexual experience like?   \n",
      "\n",
      "                                            question2  is_duplicate  \n",
      "5   I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
      "7           What should I do to be a great geologist?             1  \n",
      "11             How can I see all my Youtube comments?             1  \n",
      "12            How can you make physics easy to learn?             1  \n",
      "13             What was your first sexual experience?             1  \n",
      "\n",
      "count: \n",
      "id              404351\n",
      "qid1            404351\n",
      "qid2            404351\n",
      "question1       404350\n",
      "question2       404349\n",
      "is_duplicate    404351\n",
      "dtype: int64\n",
      "\n",
      "sum is_duplicate: 149306\n"
     ]
    }
   ],
   "source": [
    "print(df[df['is_duplicate'] == 1].head())\n",
    "print(f'\\ncount: \\n{df.count()}')\n",
    "print(f\"\\nsum is_duplicate: {df['is_duplicate'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df['is_duplicate']\n",
    "# dft = df.drop('is_duplicate', axis=1)\n",
    "dft = df\n",
    "X_train, X_test, y_train, y_test = train_test_split(dft, y, test_size=.75, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sum is_duplicate: 37296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id              101087\n",
       "qid1            101087\n",
       "qid2            101087\n",
       "question1       101087\n",
       "question2       101087\n",
       "is_duplicate    101087\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\nsum is_duplicate: {X_train['is_duplicate'].sum()}\")\n",
    "X_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              303264\n",
       "qid1            303264\n",
       "qid2            303264\n",
       "question1       303263\n",
       "question2       303262\n",
       "is_duplicate    303264\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_q1 = X_train['question1'].values\n",
    "train_q2 = X_train['question2'].values\n",
    "train_labels = X_train['is_duplicate'].values\n",
    "\n",
    "sample = X_test.sample(n=10000, random_state=42)\n",
    "test_q1 = X_test['question1'].values\n",
    "test_q2 =X_test['question1'].values\n",
    "test_labels = X_test['is_duplicate'].values\n",
    "test_ids = X_test['id'].values  # id.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-31 22:23:46.449215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-31 22:23:46.458058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-31 22:23:46.459139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-31 22:23:46.460655: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-31 22:23:46.462365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-31 22:23:46.463171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-31 22:23:46.463794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-31 22:23:46.869427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-31 22:23:46.869972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-31 22:23:46.870458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-31 22:23:46.870917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9268 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:43:00.0, compute capability: 6.1\n",
      "2021-10-31 22:23:47.895519: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index len: 75959\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text in dataset\n",
    "print('Processing text dataset')\n",
    "\n",
    "def text_to_wordlist(text):\n",
    "    \n",
    "    # split words\n",
    "    text = str(text).split()\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Use re to clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"what's\", \"what is \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\’s\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\'s\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"n't\", \" not \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\‘\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\’\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\\"\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\“\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\”\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\",\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\.\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"!\", \" ! \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\/\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\+\", \" + \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\-\", \" - \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\=\", \" = \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"'\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\":\", \" : \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text, re.IGNORECASE)\n",
    "    text = re.sub(r\" e g \", \" eg \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\" b g \", \" bg \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\" u s \", \" american \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"j k\", \"jk\", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\？\", \" \", text, re.IGNORECASE)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "train_text_q1 = [] # preprocessed text of q1\n",
    "train_text_q2 = [] # preprocessed text of q2\n",
    "\n",
    "text_set = set() # complete set of words for building embeddings\n",
    "\n",
    "for text in train_q1:\n",
    "    tt = text_to_wordlist(text)\n",
    "    text_set.add(tt)\n",
    "    train_text_q1.append(tt)\n",
    "for text in train_q2:\n",
    "    tt = text_to_wordlist(text)\n",
    "    text_set.add(tt)\n",
    "    train_text_q2.append(tt)\n",
    "\n",
    "test_text_q1 = [] # preprocessed text of q1\n",
    "test_text_q2 = [] # preprocessed text of q2\n",
    "\n",
    "for text in test_q1:\n",
    "    tt = text_to_wordlist(text)\n",
    "    text_set.add(tt)\n",
    "    test_text_q1.append(tt)\n",
    "for text in test_q2:\n",
    "    tt = text_to_wordlist(text)\n",
    "    text_set.add(tt)\n",
    "    test_text_q2.append(tt)\n",
    "\n",
    "train_test_text = list(text_set)\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=MAX_NUM_WORDS, output_sequence_length=EMBEDDING_DIM)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_test_text).batch(128)\n",
    "vectorizer.adapt(text_ds)\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "print(f'word_index len: {len(word_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74264 unique tokens are found\n",
      "Shape of train data tensor: (101087, 60)\n",
      "Shape of train labels tensor: (101087,)\n",
      "Shape of test data vtensor: (303264, 60)\n",
      "Shape of test ids tensor: (303264,)\n"
     ]
    }
   ],
   "source": [
    "# Keras.Tokenize for all text:\n",
    "# First construct a Tokenizer()\n",
    "# Then use tokenizer_on_texts() method to learn the dictionary of the corpus(all texts(sentences)). \n",
    "#    We can use .word_index to map between the each word (distinct) with the corresponding number.\n",
    "# Then use text_to_sequence() method to transfer every text(sentence) in texts into sequences of word_indexes.\n",
    "# Then add the same length by padding method: padding_sequences().\n",
    "# Finally use the embedding layer in keras to carry out a vectorization, and input it into LSTM.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_text_q1 + train_text_q2 + test_text_q1 + test_text_q2)  # generate a token dictionary, \n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train_text_q1)  # sequence of q1\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train_text_q2)  # sequence of q2\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_text_q1)  # sequence of q1_test\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_text_q2)  # sequence of q2_test\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# Pad all train with Max_Sequence_Length: 60\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q1 as train_data\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q2 as train_data\n",
    "print('Shape of train data tensor:', train_data_1.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# Pad all test with Max_Sequence_Length\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q1_test as test_data\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q2_test as test_data\n",
    "print('Shape of test data vtensor:', test_data_2.shape)\n",
    "print('Shape of test ids tensor:', test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 64618 words (9646 misses)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.layers import Dot\n",
    "\n",
    "num_tokens = len(voc) + 2\n",
    "hits = 0\n",
    "misses = 0\n",
    "misses_txt = []\n",
    "\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # print(embedding_vector.shape)\n",
    "    # if embedding_vector.shape[0] == 0:\n",
    "    #    print(f'word: {word}')\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "        misses_txt.append(word)\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "# print(f'misses: {misses_txt}')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    EMBEDDING_DIM,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")\n",
    "\n",
    "# BiLSTM layer\n",
    "from keras.layers import Bidirectional, LSTM\n",
    "lstm_layer = Bidirectional(LSTM(N_HIDDEN, dropout=DROPOUT_RATE_LSTM, recurrent_dropout=DROPOUT_RATE_LSTM))\n",
    "\n",
    "\n",
    "# Define inputs\n",
    "seq1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "seq2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# Run inputs through embedding\n",
    "emb1 = embedding_layer(seq1)\n",
    "emb2 = embedding_layer(seq2)\n",
    "\n",
    "# Run through LSTM layers\n",
    "lstm_a = lstm_layer(emb1)\n",
    "lstm_b = lstm_layer(emb2)\n",
    "\n",
    "# cosin_sim_func = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([lstm_a, lstm_b])\n",
    "dotted = Dot(axes=-1, normalize=True)([lstm_a, lstm_b])\n",
    "\n",
    "l1_norm = lambda x: 1 - K.abs(x[0] - x[1])\n",
    "l1_dist = Lambda(function=l1_norm, output_shape=lambda x: x[0], name='L1_distance')([lstm_a, lstm_b])\n",
    "\n",
    "merged = concatenate([lstm_a, lstm_b, l1_dist, dotted])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "merged = Dense(N_DENSE, activation=ACTIVE_FUNC)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the model training\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 60, 300)      22788300    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 1024)         3330048     embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "L1_distance (Lambda)            (None, 1024)         0           bidirectional[0][0]              \n",
      "                                                                 bidirectional[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           bidirectional[0][0]              \n",
      "                                                                 bidirectional[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3073)         0           bidirectional[0][0]              \n",
      "                                                                 bidirectional[1][0]              \n",
      "                                                                 L1_distance[0][0]                \n",
      "                                                                 dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3073)         12292       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 3073)         0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          786944      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 26,918,865\n",
      "Trainable params: 4,123,907\n",
      "Non-trainable params: 22,794,958\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/25\n",
      "711/711 [==============================] - 409s 565ms/step - loss: 0.5167 - acc: 0.7426 - val_loss: 0.4764 - val_acc: 0.7671\n",
      "Epoch 2/25\n",
      "711/711 [==============================] - 402s 566ms/step - loss: 0.4166 - acc: 0.8001 - val_loss: 0.4624 - val_acc: 0.7737\n",
      "Epoch 3/25\n",
      "711/711 [==============================] - 403s 568ms/step - loss: 0.3646 - acc: 0.8290 - val_loss: 0.4970 - val_acc: 0.7808\n",
      "Epoch 4/25\n",
      "711/711 [==============================] - 403s 567ms/step - loss: 0.3186 - acc: 0.8547 - val_loss: 0.3757 - val_acc: 0.8241\n",
      "Epoch 5/25\n",
      "711/711 [==============================] - 403s 567ms/step - loss: 0.2792 - acc: 0.8752 - val_loss: 0.3977 - val_acc: 0.8186\n",
      "Epoch 6/25\n",
      "711/711 [==============================] - 402s 565ms/step - loss: 0.2418 - acc: 0.8944 - val_loss: 0.4402 - val_acc: 0.8163\n",
      "Epoch 7/25\n",
      "711/711 [==============================] - 402s 566ms/step - loss: 0.2092 - acc: 0.9103 - val_loss: 0.4107 - val_acc: 0.8308\n",
      "Epoch 8/25\n",
      "711/711 [==============================] - 400s 562ms/step - loss: 0.1823 - acc: 0.9240 - val_loss: 0.4920 - val_acc: 0.8092\n",
      "Epoch 9/25\n",
      "711/711 [==============================] - 401s 564ms/step - loss: 0.1615 - acc: 0.9327 - val_loss: 0.4735 - val_acc: 0.8271\n",
      "Epoch 10/25\n",
      "711/711 [==============================] - 401s 563ms/step - loss: 0.1418 - acc: 0.9414 - val_loss: 0.5108 - val_acc: 0.8291\n",
      "Epoch 11/25\n",
      "711/711 [==============================] - 403s 567ms/step - loss: 0.1269 - acc: 0.9493 - val_loss: 0.4824 - val_acc: 0.8367\n",
      "Epoch 12/25\n",
      "711/711 [==============================] - 402s 566ms/step - loss: 0.1141 - acc: 0.9540 - val_loss: 0.5246 - val_acc: 0.8256\n",
      "Epoch 13/25\n",
      "711/711 [==============================] - 401s 564ms/step - loss: 0.1077 - acc: 0.9586 - val_loss: 0.5460 - val_acc: 0.8377\n",
      "Epoch 14/25\n",
      "711/711 [==============================] - 402s 566ms/step - loss: 0.0982 - acc: 0.9618 - val_loss: 0.5652 - val_acc: 0.8325\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# def auroc(y_true, y_pred):\n",
    "#     return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "print('Starting the model training')\n",
    "\n",
    "model = Model(inputs=[seq1, seq2], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# Summerization of model\n",
    "model.summary()\n",
    "\n",
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "bst_model_path = VERSION + '.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([train_data_1, train_data_2], train_labels, \\\n",
    "        validation_split=.1, \\\n",
    "        epochs=25, batch_size=128, shuffle=True, \\\n",
    "        callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path) # sotre model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'embedding misses: {misses_txt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dup-question",
   "language": "python",
   "name": "dup-question"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
