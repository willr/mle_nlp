{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import unidecode\n",
    "# import contractions\n",
    "import re\n",
    "# from word2number import w2n\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "# Keras package\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, LSTM, Lambda, Bidirectional\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize global variables\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 60  \n",
    "MAX_NUM_WORDS = 200000  # There are about 201000 unique words in training dataset, 200000 is enough for tokenization\n",
    "EMBEDDING_DIM = 300  # word-embedded-vector dimension(300 is for 'glove.42B.300d')\n",
    "N_HIDDEN = 512\n",
    "N_DENSE = 256\n",
    "\n",
    "DROPOUT_RATE_LSTM = 0.10 # drop-out possibility, random set to avoid outfitting\n",
    "DROPOUT_RATE_DENSE = 0.15\n",
    "\n",
    "ACTIVE_FUNC = 'relu'\n",
    "VERSION = 'bilstm'\n",
    "\n",
    "PATH_TO_GLOVE_FILE = './data/glove.42B.300d.txt'\n",
    "# PATH_TO_GLOVE_FILE = './data/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes Hidden: 512\n",
      "Nodes Dense: 256\n",
      "Dropout Rate LSTM: 0.1\n",
      "Dropout Rate Dense: 0.15\n"
     ]
    }
   ],
   "source": [
    "print(f'Nodes Hidden: {N_HIDDEN}')\n",
    "print(f'Nodes Dense: {N_DENSE}')\n",
    "\n",
    "print(f'Dropout Rate LSTM: {DROPOUT_RATE_LSTM}')\n",
    "print(f'Dropout Rate Dense: {DROPOUT_RATE_DENSE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create word embedding dictionary\n",
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Create word embedding dictionary from 'glove.840B.300d.txt', {key:value} is {word: glove vector(300,)}\n",
    "print('Create word embedding dictionary')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(PATH_TO_GLOVE_FILE) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/questions.csv.zip')\n",
    "df.head()\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  qid1  qid2                                          question1  \\\n",
      "5    5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
      "7    7    15    16                     How can I be a good geologist?   \n",
      "11  11    23    24        How do I read and find my YouTube comments?   \n",
      "12  12    25    26               What can make Physics easy to learn?   \n",
      "13  13    27    28        What was your first sexual experience like?   \n",
      "\n",
      "                                            question2  is_duplicate  \n",
      "5   I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
      "7           What should I do to be a great geologist?             1  \n",
      "11             How can I see all my Youtube comments?             1  \n",
      "12            How can you make physics easy to learn?             1  \n",
      "13             What was your first sexual experience?             1  \n",
      "\n",
      "count: \n",
      "id              404351\n",
      "qid1            404351\n",
      "qid2            404351\n",
      "question1       404350\n",
      "question2       404349\n",
      "is_duplicate    404351\n",
      "dtype: int64\n",
      "\n",
      "sum is_duplicate: 149306\n"
     ]
    }
   ],
   "source": [
    "print(df[df['is_duplicate'] == 1].head())\n",
    "print(f'\\ncount: \\n{df.count()}')\n",
    "print(f\"\\nsum is_duplicate: {df['is_duplicate'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df['is_duplicate']\n",
    "# dft = df.drop('is_duplicate', axis=1)\n",
    "dft = df\n",
    "X_train, X_test, y_train, y_test = train_test_split(dft, y, test_size=.75, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sum is_duplicate: 37296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id              101087\n",
       "qid1            101087\n",
       "qid2            101087\n",
       "question1       101087\n",
       "question2       101087\n",
       "is_duplicate    101087\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\nsum is_duplicate: {X_train['is_duplicate'].sum()}\")\n",
    "X_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              303264\n",
       "qid1            303264\n",
       "qid2            303264\n",
       "question1       303263\n",
       "question2       303262\n",
       "is_duplicate    303264\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_q1 = X_train['question1'].values\n",
    "train_q2 = X_train['question2'].values\n",
    "train_labels = X_train['is_duplicate'].values\n",
    "\n",
    "sample = X_test.sample(n=10000, random_state=42)\n",
    "test_q1 = X_test['question1'].values\n",
    "test_q2 =X_test['question1'].values\n",
    "test_labels = X_test['is_duplicate'].values\n",
    "test_ids = X_test['id'].values  # id.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "word_index len: 75959\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text in dataset\n",
    "print('Processing text dataset')\n",
    "\n",
    "def text_to_wordlist(text):\n",
    "    \n",
    "    # split words\n",
    "    text = str(text).split()\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Use re to clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"what's\", \"what is \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\’s\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\'s\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"n't\", \" not \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\‘\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\’\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\\"\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\“\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\”\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\",\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\.\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"!\", \" ! \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\/\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\+\", \" + \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\-\", \" - \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\=\", \" = \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"'\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\":\", \" : \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text, re.IGNORECASE)\n",
    "    text = re.sub(r\" e g \", \" eg \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\" b g \", \" bg \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\" u s \", \" american \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"j k\", \"jk\", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text, re.IGNORECASE)\n",
    "    text = re.sub(r\"\\？\", \" \", text, re.IGNORECASE)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "train_text_q1 = [] # preprocessed text of q1\n",
    "train_text_q2 = [] # preprocessed text of q2\n",
    "\n",
    "text_set = set() # complete set of words for building embeddings\n",
    "\n",
    "for text in train_q1:\n",
    "    tt = text_to_wordlist(text)\n",
    "    text_set.add(tt)\n",
    "    train_text_q1.append(tt)\n",
    "for text in train_q2:\n",
    "    tt = text_to_wordlist(text)\n",
    "    text_set.add(tt)\n",
    "    train_text_q2.append(tt)\n",
    "\n",
    "test_text_q1 = [] # preprocessed text of q1\n",
    "test_text_q2 = [] # preprocessed text of q2\n",
    "\n",
    "for text in test_q1:\n",
    "    tt = text_to_wordlist(text)\n",
    "    text_set.add(tt)\n",
    "    test_text_q1.append(tt)\n",
    "for text in test_q2:\n",
    "    tt = text_to_wordlist(text)\n",
    "    text_set.add(tt)\n",
    "    test_text_q2.append(tt)\n",
    "\n",
    "train_test_text = list(text_set)\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=MAX_NUM_WORDS, output_sequence_length=EMBEDDING_DIM)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_test_text).batch(128)\n",
    "vectorizer.adapt(text_ds)\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "print(f'word_index len: {len(word_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74264 unique tokens are found\n",
      "Shape of train data tensor: (101087, 60)\n",
      "Shape of train labels tensor: (101087,)\n",
      "Shape of test data vtensor: (303264, 60)\n",
      "Shape of test ids tensor: (303264,)\n"
     ]
    }
   ],
   "source": [
    "# Keras.Tokenize for all text:\n",
    "# First construct a Tokenizer()\n",
    "# Then use tokenizer_on_texts() method to learn the dictionary of the corpus(all texts(sentences)). \n",
    "#    We can use .word_index to map between the each word (distinct) with the corresponding number.\n",
    "# Then use text_to_sequence() method to transfer every text(sentence) in texts into sequences of word_indexes.\n",
    "# Then add the same length by padding method: padding_sequences().\n",
    "# Finally use the embedding layer in keras to carry out a vectorization, and input it into LSTM.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_text_q1 + train_text_q2 + test_text_q1 + test_text_q2)  # generate a token dictionary, \n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train_text_q1)  # sequence of q1\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train_text_q2)  # sequence of q2\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_text_q1)  # sequence of q1_test\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_text_q2)  # sequence of q2_test\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# Pad all train with Max_Sequence_Length: 60\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q1 as train_data\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q2 as train_data\n",
    "print('Shape of train data tensor:', train_data_1.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# Pad all test with Max_Sequence_Length\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q1_test as test_data\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)  # padded_sequence of q2_test as test_data\n",
    "print('Shape of test data vtensor:', test_data_2.shape)\n",
    "print('Shape of test ids tensor:', test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 64618 words (9646 misses)\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "hits = 0\n",
    "misses = 0\n",
    "misses_txt = []\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # print(embedding_vector.shape)\n",
    "    # if embedding_vector.shape[0] == 0:\n",
    "    #    print(f'word: {word}')\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "        misses_txt.append(word)\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "# print(f'misses: {misses_txt}')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    EMBEDDING_DIM,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")\n",
    "\n",
    "# BiLSTM layer\n",
    "from keras.layers import Bidirectional, LSTM\n",
    "lstm_layer = Bidirectional(LSTM(N_HIDDEN, dropout=DROPOUT_RATE_LSTM, recurrent_dropout=DROPOUT_RATE_LSTM))\n",
    "\n",
    "\n",
    "# Define inputs\n",
    "seq1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "seq2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# Run inputs through embedding\n",
    "emb1 = embedding_layer(seq1)\n",
    "emb2 = embedding_layer(seq2)\n",
    "\n",
    "# Run through LSTM layers\n",
    "lstm_a = lstm_layer(emb1)\n",
    "lstm_b = lstm_layer(emb2)\n",
    "\n",
    "merged = concatenate([lstm_a, lstm_b])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "merged = Dense(N_DENSE, activation=ACTIVE_FUNC)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(DROPOUT_RATE_DENSE)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the model training\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 60, 300)      22788300    input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1024)         3330048     embedding_3[0][0]                \n",
      "                                                                 embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 2048)         0           bidirectional_3[0][0]            \n",
      "                                                                 bidirectional_3[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 2048)         8192        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 2048)         0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 256)          524544      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256)          1024        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            257         dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 26,652,365\n",
      "Trainable params: 3,859,457\n",
      "Non-trainable params: 22,792,908\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/25\n",
      "711/711 [==============================] - 430s 594ms/step - loss: 0.5695 - acc: 0.7037 - val_loss: 0.8579 - val_acc: 0.6378\n",
      "Epoch 2/25\n",
      "711/711 [==============================] - 411s 577ms/step - loss: 0.5144 - acc: 0.7403 - val_loss: 0.4807 - val_acc: 0.7584\n",
      "Epoch 3/25\n",
      "711/711 [==============================] - 416s 584ms/step - loss: 0.4734 - acc: 0.7660 - val_loss: 0.4643 - val_acc: 0.7661\n",
      "Epoch 4/25\n",
      "711/711 [==============================] - 422s 593ms/step - loss: 0.4389 - acc: 0.7865 - val_loss: 0.4453 - val_acc: 0.7886\n",
      "Epoch 5/25\n",
      "711/711 [==============================] - 414s 582ms/step - loss: 0.4045 - acc: 0.8083 - val_loss: 0.4316 - val_acc: 0.7929\n",
      "Epoch 6/25\n",
      "711/711 [==============================] - 415s 584ms/step - loss: 0.3683 - acc: 0.8290 - val_loss: 0.4351 - val_acc: 0.7937\n",
      "Epoch 7/25\n",
      "711/711 [==============================] - 415s 584ms/step - loss: 0.3260 - acc: 0.8523 - val_loss: 0.4362 - val_acc: 0.8004\n",
      "Epoch 8/25\n",
      "711/711 [==============================] - 421s 592ms/step - loss: 0.2855 - acc: 0.8718 - val_loss: 0.4582 - val_acc: 0.8006\n",
      "Epoch 9/25\n",
      "711/711 [==============================] - 415s 583ms/step - loss: 0.2459 - acc: 0.8929 - val_loss: 0.4609 - val_acc: 0.8062\n",
      "Epoch 10/25\n",
      "711/711 [==============================] - 420s 590ms/step - loss: 0.2121 - acc: 0.9104 - val_loss: 0.4851 - val_acc: 0.8044\n",
      "Epoch 11/25\n",
      "711/711 [==============================] - 419s 589ms/step - loss: 0.1825 - acc: 0.9244 - val_loss: 0.5374 - val_acc: 0.8043\n",
      "Epoch 12/25\n",
      "711/711 [==============================] - 412s 580ms/step - loss: 0.1582 - acc: 0.9344 - val_loss: 0.5940 - val_acc: 0.7998\n",
      "Epoch 13/25\n",
      "711/711 [==============================] - 416s 585ms/step - loss: 0.1397 - acc: 0.9443 - val_loss: 0.5956 - val_acc: 0.7976\n",
      "Epoch 14/25\n",
      "711/711 [==============================] - 419s 589ms/step - loss: 0.1216 - acc: 0.9512 - val_loss: 0.6313 - val_acc: 0.8022\n",
      "Epoch 15/25\n",
      "711/711 [==============================] - 418s 588ms/step - loss: 0.1104 - acc: 0.9568 - val_loss: 0.6422 - val_acc: 0.8025\n",
      "Epoch 16/25\n",
      "711/711 [==============================] - 415s 584ms/step - loss: 0.0982 - acc: 0.9620 - val_loss: 0.7211 - val_acc: 0.8080\n",
      "Epoch 17/25\n",
      "711/711 [==============================] - 422s 594ms/step - loss: 0.0923 - acc: 0.9644 - val_loss: 0.7066 - val_acc: 0.8039\n",
      "Epoch 18/25\n",
      "711/711 [==============================] - 415s 584ms/step - loss: 0.0829 - acc: 0.9683 - val_loss: 0.7401 - val_acc: 0.8041\n",
      "Epoch 19/25\n",
      "711/711 [==============================] - 414s 583ms/step - loss: 0.0761 - acc: 0.9713 - val_loss: 0.7507 - val_acc: 0.8079\n",
      "Epoch 20/25\n",
      "711/711 [==============================] - 419s 589ms/step - loss: 0.0720 - acc: 0.9724 - val_loss: 0.8127 - val_acc: 0.8041\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "print('Starting the model training')\n",
    "\n",
    "model = Model(inputs=[seq1, seq2], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# Summerization of model\n",
    "model.summary()\n",
    "\n",
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=15)\n",
    "bst_model_path = VERSION + '.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "\n",
    "hist = model.fit([train_data_1, train_data_2], train_labels, \\\n",
    "        validation_split=.1, \\\n",
    "        epochs=25, batch_size=128, shuffle=True, \\\n",
    "        callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path) # sotre model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'embedding misses: {misses_txt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dup-question",
   "language": "python",
   "name": "dup-question"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
